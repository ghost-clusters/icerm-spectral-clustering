{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.spectral_clustering import spectral_clustering\n",
    "from lib.categorical_similarity_functions import categorical_preprocessing_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing's first, we need to take our csv file and make it something that we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfAtts, dataA = categorical_preprocessing_csv(\"lib/data/mushrooms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take that data and cluster it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please be a real thing:  [[1.         0.96085267 0.95796875 ... 0.9229095  0.94593881 0.92519783]\n",
      " [0.96085267 1.         0.99351086 ... 0.95104249 0.91610079 0.95333082]\n",
      " [0.95796875 0.99351086 1.         ... 0.95163808 0.91610079 0.95104249]\n",
      " ...\n",
      " [0.9229095  0.95104249 0.95163808 ... 1.         0.91495269 0.99606841]\n",
      " [0.94593881 0.91610079 0.91610079 ... 0.91495269 1.         0.91495269]\n",
      " [0.92519783 0.95333082 0.95104249 ... 0.99606841 0.91495269 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "assns = spectral_clustering(data,2,\"rw\",numOfAtts=numOfAtts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(assns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! We have our assignments! Now, we need to be able to compare these assignments to the actual data (we want to see how well we clustered the data into poisonous and edible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p' 'e' 'e' ... 'e' 'p' 'e']\n",
      "(8124,)\n"
     ]
    }
   ],
   "source": [
    "dataT = np.array(data.T)\n",
    "verify = dataT[0] # @Max I called it verify because I want to verify the clusters\n",
    "print(verify)\n",
    "print(verify.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken the first column and reshaped the data, we can convert our 'p''s to 1's and 'e''s to 0's so we can compare this vector to our assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "bindata = [] # @Max I called it bindata because i'm taking the data and converting it to 0's and 1's, like binary\n",
    "for i in range(len(verify)):\n",
    "    if verify[i]=='p':\n",
    "        bindata.append(0)\n",
    "    elif verify[i] == 'e':\n",
    "        bindata.append(1)\n",
    "bindata = np.array(bindata)\n",
    "print(bindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll compute the error by taking the norm of the difference between the cluster assignments vector and the actual categorical assignments of the mushrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002892008893271967\n"
     ]
    }
   ],
   "source": [
    "errvec = assns-bindata # @Max I called it errvec because i'm going to use it to compute the error\n",
    "n= errvec.shape[0]\n",
    "err = np.linalg.norm(errvec)/n\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does pretty well! But it's also worth noting that we use the column that tells us whether a mushroom is poisonous or not to make our clusters. To get a more accurate representation of how well our clustering algorithm works, we should ignore this column when we cluster and then make the same comparison as we did here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8124, 22)\n",
      "(22,)\n"
     ]
    }
   ],
   "source": [
    "data = data.T[1:]\n",
    "data = data.T\n",
    "numOfAtts = numOfAtts[1:]\n",
    "print(data.shape)\n",
    "print(numOfAtts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please be a real thing:  [[1.         0.97422476 0.97120975 ... 0.9345569  0.94348149 0.93694924]\n",
      " [0.97422476 1.         0.9932159  ... 0.94881715 0.92743871 0.95120949]\n",
      " [0.97120975 0.9932159  1.         ... 0.94943981 0.92743871 0.94881715]\n",
      " ...\n",
      " [0.9345569  0.94881715 0.94943981 ... 1.         0.92623842 0.9958897 ]\n",
      " [0.94348149 0.92743871 0.92743871 ... 0.92623842 1.         0.92623842]\n",
      " [0.93694924 0.95120949 0.94881715 ... 0.9958897  0.92623842 1.        ]]\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "assns, SandU = spectral_clustering(data,2,\"rw\", with_eigen = True, numOfAtts=numOfAtts)\n",
    "print(assns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00567826814114761\n",
      "5996\n"
     ]
    }
   ],
   "source": [
    "dataT = np.array(dataA.T)\n",
    "verify = dataT[0] \n",
    "bindata = [] \n",
    "for i in range(len(verify)):\n",
    "    if verify[i]=='p':\n",
    "        bindata.append(0)\n",
    "    elif verify[i] == 'e':\n",
    "        bindata.append(1)\n",
    "bindata = np.array(bindata)\n",
    "errvec = assns-bindata \n",
    "n= errvec.shape[0]\n",
    "err = np.linalg.norm(errvec)/n\n",
    "print(err)\n",
    "\n",
    "# to check in how many spots the assingments match reality\n",
    "count=0\n",
    "for i in range(len(errvec)):\n",
    "    if errvec[i]==0:\n",
    "        count +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the error is nice and all, but we all know that visuals are where it's at. The only problem is that our data is HUGE. So, why don't we take advantage of the dimension reduction done by spectral clustering, and project our data into \\mathbb{R}^2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p' 'e' 'e' 'p' 'e' 'e' 'e' 'e' 'p' 'e' 'e' 'e' 'e' 'p' 'e' 'e' 'e' 'p'\n",
      " 'p' 'p' 'e' 'p' 'e' 'e' 'e' 'p' 'e' 'e' 'e' 'e']\n",
      "[0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1]\n",
      "[1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      "(8124, 2)\n",
      "[4.01819435e-15 9.86980042e-01]\n"
     ]
    }
   ],
   "source": [
    "print(verify[0:30])\n",
    "print(bindata[0:30])\n",
    "print(assns[0:30])\n",
    "S, U = SandU\n",
    "print(U.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this crap...it's bad and makes no sense i was tripping on shrooms :)\n",
    "print(SandU)\n",
    "S, U = SandU\n",
    "\n",
    "proj_of_d_on_u = []\n",
    "i = 0\n",
    "u0 = U[:,0]\n",
    "u1 = U[:,1]\n",
    "d = data[i,:]\n",
    "u0norm = np.sqrt(sum(u0**2))\n",
    "u1norm = np.sqrt(sum(u1**2))\n",
    "proj = ((np.dot(d,u0)/(u0norm**2))*u0)+((np.dot(d,u1)/(u1norm**2))*u1)\n",
    "\n",
    "print(proj)\n",
    "\n",
    "proj_of_d_on_u.append(proj)\n",
    "\n",
    "for i in range of len(U):\n",
    "    ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
