{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-04637a061d22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from lib.spectral_clustering import spectral_clustering\n",
    "from lib.categorical_similarity_functions import categorical_preprocessing_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing's first, we need to take our csv file and make it something that we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lib/data/mushrooms.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9a20820681e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnumOfAtts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_preprocessing_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lib/data/mushrooms.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\summer@ICERM\\icerm-spectral-clustering\\lib\\categorical_similarity_functions.py\u001b[0m in \u001b[0;36mcategorical_preprocessing_csv\u001b[1;34m(datacsv)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Outputs the number of attributes for each categorical variable and the data without headings as a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcategorical_preprocessing_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatacsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatacsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mso_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lib/data/mushrooms.csv'"
     ]
    }
   ],
   "source": [
    "numOfAtts, dataA = categorical_preprocessing_csv(\"lib/data/mushrooms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take that data and cluster it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please be a real thing:  [[1.         0.96085267 0.95796875 ... 0.9229095  0.94593881 0.92519783]\n",
      " [0.96085267 1.         0.99351086 ... 0.95104249 0.91610079 0.95333082]\n",
      " [0.95796875 0.99351086 1.         ... 0.95163808 0.91610079 0.95104249]\n",
      " ...\n",
      " [0.9229095  0.95104249 0.95163808 ... 1.         0.91495269 0.99606841]\n",
      " [0.94593881 0.91610079 0.91610079 ... 0.91495269 1.         0.91495269]\n",
      " [0.92519783 0.95333082 0.95104249 ... 0.99606841 0.91495269 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "assns = spectral_clustering(data,2,\"rw\",numOfAtts=numOfAtts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(assns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! We have our assignments! Now, we need to be able to compare these assignments to the actual data (we want to see how well we clustered the data into poisonous and edible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p' 'e' 'e' ... 'e' 'p' 'e']\n",
      "(8124,)\n"
     ]
    }
   ],
   "source": [
    "dataT = np.array(data.T)\n",
    "verify = dataT[0] # @Max I called it verify because I want to verify the clusters\n",
    "print(verify)\n",
    "print(verify.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken the first column and reshaped the data, we can convert our 'p''s to 1's and 'e''s to 0's so we can compare this vector to our assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "bindata = [] # @Max I called it bindata because i'm taking the data and converting it to 0's and 1's, like binary\n",
    "for i in range(len(verify)):\n",
    "    if verify[i]=='p':\n",
    "        bindata.append(0)\n",
    "    elif verify[i] == 'e':\n",
    "        bindata.append(1)\n",
    "bindata = np.array(bindata)\n",
    "print(bindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll compute the error by taking the norm of the difference between the cluster assignments vector and the actual categorical assignments of the mushrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002892008893271967\n"
     ]
    }
   ],
   "source": [
    "errvec = assns-bindata # @Max I called it errvec because i'm going to use it to compute the error\n",
    "n= errvec.shape[0]\n",
    "err = np.linalg.norm(errvec)/n\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does pretty well! But it's also worth noting that we use the column that tells us whether a mushroom is poisonous or not to make our clusters. To get a more accurate representation of how well our clustering algorithm works, we should ignore this column when we cluster and then make the same comparison as we did here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8124, 22)\n",
      "(22,)\n"
     ]
    }
   ],
   "source": [
    "data = dataA.T[1:]\n",
    "data = data.T\n",
    "numOfAtts = numOfAtts[1:]\n",
    "print(data.shape)\n",
    "print(numOfAtts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please be a real thing:  [[1.         0.97422476 0.97120975 ... 0.9345569  0.94348149 0.93694924]\n",
      " [0.97422476 1.         0.9932159  ... 0.94881715 0.92743871 0.95120949]\n",
      " [0.97120975 0.9932159  1.         ... 0.94943981 0.92743871 0.94881715]\n",
      " ...\n",
      " [0.9345569  0.94881715 0.94943981 ... 1.         0.92623842 0.9958897 ]\n",
      " [0.94348149 0.92743871 0.92743871 ... 0.92623842 1.         0.92623842]\n",
      " [0.93694924 0.95120949 0.94881715 ... 0.9958897  0.92623842 1.        ]]\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "assns, SandU = spectral_clustering(data,2,\"rw\", with_eigen = True, numOfAtts=numOfAtts)\n",
    "print(assns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00567826814114761\n",
      "5996\n"
     ]
    }
   ],
   "source": [
    "dataT = np.array(dataA.T)\n",
    "verify = dataT[0] \n",
    "bindata = [] \n",
    "for i in range(len(verify)):\n",
    "    if verify[i]=='p':\n",
    "        bindata.append(0)\n",
    "    elif verify[i] == 'e':\n",
    "        bindata.append(1)\n",
    "bindata = np.array(bindata)\n",
    "errvec = assns-bindata \n",
    "n= errvec.shape[0]\n",
    "err = np.linalg.norm(errvec)/n\n",
    "print(err)\n",
    "\n",
    "# to check in how many spots the assingments match reality\n",
    "count=0\n",
    "for i in range(len(errvec)):\n",
    "    if errvec[i]==0:\n",
    "        count +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the error is nice and all, but we all know that visuals are where it's at. The only problem is that our data is HUGE. So, why don't we take advantage of the dimension reduction done by spectral clustering, and project our data into $\\mathbb{R}^2$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p' 'e' 'e' 'p' 'e' 'e' 'e' 'e' 'p' 'e' 'e' 'e' 'e' 'p' 'e' 'e' 'e' 'p'\n",
      " 'p' 'p' 'e' 'p' 'e' 'e' 'e' 'p' 'e' 'e' 'e' 'e']\n",
      "[0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1]\n",
      "[1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      "(8124, 2)\n",
      "[4.01819435e-15 9.86980042e-01]\n"
     ]
    }
   ],
   "source": [
    "print(verify[0:30])\n",
    "print(bindata[0:30])\n",
    "print(assns[0:30])\n",
    "S, U = SandU\n",
    "print(U.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent our clusters, we use PCA to find the k dominant features of our 22-dimensional data. To do this, we need to first normalize our data matrix so that it has mean 0 and variance 1 (depending on if we think features are independent or dependent of the variance). Then we find the k eigenvectors corresponding to the largest k eigenvalues of the covariance matrix $\\frac{1}{n}X^TX$. These k eigenvectors are what we call the principal components and maximize the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row is a mushroom\n",
    "# center each column to 0\n",
    "rows, column = np.shape(data)\n",
    "means_of_columns = np.mean(data, axis = 0) #22 * 1 vector of means\n",
    "normalized_data = np.zeros(np.shape(data)).T #22*8000 matrix\n",
    "# center each coordinate of data\n",
    "for i in range(columns):\n",
    "    normalized_data[i] = data.T[i] - means_of_columns[i] \n",
    "pca = PCA(n_components = 2) #if we want 2 components\n",
    "pca.fit(X)\n",
    "components = pca.components_ #2 vectors corresponding to the principle axes\n",
    "\n",
    "# project each data point onto the 2 components\n",
    "projection_matrix = np.array(components).T #columns are the components\n",
    "projected_data = data @ projection_matrix #best representation of 22-dim in 2-dim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
