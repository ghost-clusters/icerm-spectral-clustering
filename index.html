<!DOCTYPE html>
	<style>
		@import url('https://fonts.googleapis.com/css?family=Crimson+Text:400|Avenir:400,700&display=swap');
		.container {
			margin: auto 10%;
			font-family: 'Avenir', sans-serif;
			
		}

		h1, h2, h3, h4 {
			font-family: 'Avenir', serif;
			font-weight: 400;
			margin-bottom: 0.5rem;
		}
		h1 {
			font-size: 2.5rem;
		}

		b {
			font-weight: 700;
		}


	</style>

	<body>
		<div class="container">
			<h1>Summer@ICERM 2020: Graph Spectral Clustering</h1>
			<table style="width: 80%">
				<tr>
					<td>
					<b>Max Daniels</b> <br />
					<span style="font-size: 12px; color: #808080">
						Northeastern Univeristy
						<br />
						daniels.g@northeastern.edu
					</span>
					</td>
					<td>
					<b>Catherine Huang</b> <br />
					<span style="font-size: 12px; color: #808080">
						University of California, Berkeley
						<br />
						thecatherinehuang@berkeley.edu
					</span>
					</td>
					<td>
					<b>Shubham Makaria</b> <br />
					<span style="font-size: 12px; color: #808080">
						Brown University	
						<br />
						shubham@brown.edu
					</span>
					</td>
					<td>
					<b>Chloe Makdad</b> <br />
					<span style="font-size: 12px; color: #808080">
						Butler University	
						<br />
						cmakdad@butler.edu
					</span>
					</td>
				</tr>
			</table>
		<br />
		<p>Welcome to the project page for our Summer@ICERM 2020 project! </p>
		
		<p> We studied the Graph Spectral Clustering algorithm, an alternative to Euclidean clustering algorithms like K-Means, which is geometrically motivated for use on data with manifold structure. It is often challenging to cluster manifold structured data because the Euclidean distance between any two points may not take into account their "true" distance along a geodesic path within the data manifold. A useful rule of thumb is that in high dimensional settings, only local Euclidean distances are meaningful. </p>

		<p>Spectral clustering takes this into account by operating on a similarity graph derived from the data. Edges in the graph represent data similarity and are typically computed with a kernel function like RBF or the exponential kernel. Notice that this metric prioritizes local Euclidian distances over global distances due to exponential decay. After obtaining the similarity graph, Graph Spectral Clustering utilizes a derived matrix, the <em>Graph Laplacian</em> to find cluster assignments.</p>

		<p>In this work, we study the role of the Graph Laplacian and connect Graph Spectral Clustering to the behavior of <em>heat diffusion</em>, which is governed by a second order PDE involving the Laplacian operator. For more information, please see <a href="#">our report</a>!</p>
		
		<p><em>We thank our project supervisors, Akil Narayan and Minah Oh, as well as our graduate TA supervisors, Alex Mihai and Liu Yang, for their valuable insight throughout this program. We also thank our peers at ICERM for useful discussion, and all of the ICERM organizers for a hosting a wonderful program.</em></p>
		<h1>Demonstrations</h1>
		<img width="80%" style="margin: 2% 10%" src="html/demo/Eigenvector_Evolution_BrokenCircle.gif" />
		<p style="margin: 0% 10%;font-size: 12px; color: #808080" width="80%">The domain is a circle graph with two equidistant low-weight edges. In the first row, we show consecutive eigenvectors of the Graph Laplacian. In the second row, we show how diffusion of random initial conditions converges to eigenvectors of the Graph Laplacian. Different columns show the solution projected onto increasingly high eigenvectors (ie. "high frequencies"). The third row shows energy remaining after this projection.</p>
		<br />
		<p>You can see our work in some of the following Jupyter notebooks:
			<ul>
				<li><p>We explored how graph spectral clustering works.  </p>
				<ul>
					<li><a href="html/Cumulative Eigenvector Clustering.html">Cumulative Eigenvector Clustering:</a> Graphs showing how eigenvectors help with picking out clusters and gives an example where GSC outperforms k-means. 				
					<li><a href="html/Random Walks Between Clusters.html">Random Walks Between Clusters:</a> Simulation showing that a random walker on a graph with transition probabilities that proportional to the weight of an edge tends to stay within clusters and rarely jumps across clusters.
				</ul>
				
				<li> <p>Since the graph Laplacian is intimately related to the Laplace operator, we decided to look at some partial differential equations that included the Laplace operator. The first equation we investigated was the heat equation. </p>
				<ul>
					<li><a href="html/Inverse Laplacian and the Heat Kernel.html">Inverse Laplacian and the Heat Kernel:</a> Connecting the Green's function of the heat equation with the gaussian similarity metric (solution to the heat equation) we choose to use in the Laplacian matrix. 
					<li><a href="html/Evolving Eigenvectors under the Heat Equation.html">Evolving Eigenvectors under the Heat Equation:</a> Shows how the heat equation picks out the eigenvector corresponding to the smallest eigenvalue.
					<li><a href="html/Clustering and Image Blurs.html">Clustering and Image Blurs:</a> Shows how heat diffusion relates to clustering sections of an image, in particular differentiating the foreground and background.
				</ul>
				
				<li><p>The next PDE we investigated was the wave equation.</p>
				<ul>
					<li><a href="html/Hearing the Clusters of a Graph.html">Hearing the Clusters of a Graph:</a> Shows how evolving a graph with random initial conditions according to the wave equation could allow us to recover eigenvectors.
				</ul>
				
				<li> <p>We also applied this algorithm on some real world datasets. </p>
				<ul>
					<li><a href="html/Mushroom_Clustering.html">Mushroom_Clustering:</a> Applied graph spectral clustering on a categorical dataset. 
					<li><a href="html/GSC for Codenames.html">GSC for Codenames</a> Applied graph spectral clustering to a word vector embedding to use for popular board game Codenames.
				</ul>
			</ul>
		</p>
		</div>

		
	</body>

</html>
