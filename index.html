<!DOCTYPE html>
	<style>
		@import url('https://fonts.googleapis.com/css?family=Crimson+Text:400|Avenir:400,700&display=swap');
		.container {
			margin: auto 10%;
			font-family: 'Avenir', sans-serif;
			
		}

		h1, h2, h3, h4 {
			font-family: 'Avenir', serif;
			font-weight: 400;
			margin-bottom: 0.5rem;
		}
		h1 {
			font-size: 2.5rem;
		}

		b {
			font-weight: 700;
		}


	</style>

	<body>
		<div class="container">
			<h1>Summer@ICERM 2020: Graph Spectral Clustering</h1>
			<table style="width: 80%">
				<tr>
					<td>
					<b>Max Daniels</b> <br />
					<span style="font-size: 12px; color: #808080">
						Northeastern Univeristy
						<br />
						daniels.g@northeastern.edu
					</span>
					</td>
					<td>
					<b>Catherine Huang</b> <br />
					<span style="font-size: 12px; color: #808080">
						University of California, Berkeley
						<br />
						thecatherinehuang@berkeley.edu
					</span>
					</td>
					<td>
					<b>Shubham Makharia</b> <br />
					<span style="font-size: 12px; color: #808080">
						Brown University	
						<br />
						shubham@brown.edu
					</span>
					</td>
					<td>
					<b>Chloe Makdad</b> <br />
					<span style="font-size: 12px; color: #808080">
						Butler University	
						<br />
						cmakdad@butler.edu
					</span>
					</td>
				</tr>
			</table>
		<br />
		<p>Welcome to the project page for our Summer@ICERM 2020 project! </p>
		
		<p>Clustering and dimensionality reduction are two useful tools for analyzing high dimensional datasets. Both reduce the complexity of a given dataset, either by grouping many points together or by reducing the number of data features. If the dataset takes continuous values, it can be viewed as a set of points in a high dimensional vector space and clustered based on Euclidean distance. This paradigm underlies popular clustering algorithms like K-Means, which can be inneffective on data with few intrinsic factors of variation compared to the number of features. For such data, having few factors of variation means that only a few directions in Euclidean space induce meaningful differences between data, while Euclidean distance treats all directions equally.</p>

		<p>We study the Graph Spectral Clustering algorithm, an alternative to Euclidean clustering algorithms like K-Means, which is geometrically motivated for use on data with few intrinsic factors of variation. To compare points in the dataset, the algorithm operates on a similarity graph whose edges represent similarity between two points. After obtaining the similarity graph, Graph Spectral Clustering utilizes a derived matrix, the <em>Graph Laplacian</em>.</p>

		<p>In this work, we study the role of the Graph Laplacian and connect Graph Spectral Clustering to the behavior of <em>heat diffusion</em>, which is governed by a second order PDE involving the Laplacian operator. For more information, please see <a href="html/Graph Spectral Clustering.pdf">our report</a>!</p>
		
		<p><em>We thank our project supervisors, Akil Narayan and Minah Oh, as well as our graduate TA supervisors, Alex Mihai and Liu Yang. We also thank our peers at ICERM for useful discussion, and all of the ICERM organizers for a hosting a wonderful program.</em></p>
		<h1>Demonstrations</h1>
		<img width="80%" style="margin: 2% 10%" src="html/demo/Eigenvector_Evolution_BrokenCircle.gif" />
		<p style="margin: 0% 10%;font-size: 12px; color: #808080" width="80%">The domain is a circle graph with two equidistant low-weight edges. In the first row, we show consecutive eigenvectors of the Graph Laplacian. In the second row, we show how diffusion of random initial conditions converges to eigenvectors of the Graph Laplacian. Different columns show the solution projected onto increasingly high eigenvectors (ie. "high frequencies"). The third row shows energy remaining after this projection.</p>
		<br />
		<p>You can see our work in some of the following Jupyter notebooks:
			<ul>
				<li><a href="html/Evolving Eigenvectors under the Heat Equation.html">Evolving Eigenvectors under the Heat Equation</a>
				<li><a href="html/Cumulative Eigenvector Clustering.html">Cumulative Eigenvector Clustering</a>
				<li><a href="html/Random Walks Between Clusters.html">Random Walks Between Clusters</a>
				<li><a href="html/Logistic Regression for Feature Selection.html">Logistic Regression for Feature Selection</a>
				<li><a href="html/Mushroom_Clustering.html">Mushroom_Clustering</a>
				<li><a href="html/Inverse Laplacian and the Heat Kernel.html">Inverse Laplacian and the Heat Kernel</a>
				<li><a href="html/Clustering and Image Blurs.html">Clustering and Image Blurs</a>
				<li><a href="html/1D Heat Diffusion on a Rod.html">1D Heat Diffusion on a Rod</a>
				<li><a href="html/Hearing the Clusters of a Graph.html">Hearing the Clusters of a Graph</a>
				<li><a href="html/GSC for Codenames.html">GSC for Codenames</a>
			</ul>
		</p>
		</div>

		
	</body>

</html>
