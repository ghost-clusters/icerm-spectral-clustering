<!DOCTYPE html>
	<style>
		@import url('https://fonts.googleapis.com/css?family=Crimson+Text:400|Avenir:400,700&display=swap');
		.container {
			margin: auto 10%;
			font-family: 'Avenir', sans-serif;
			
		}

		h1, h2, h3, h4 {
			font-family: 'Avenir', serif;
			font-weight: 400;
			margin-bottom: 0.5rem;
		}
		h1 {
			font-size: 2.5rem;
		}

		b {
			font-weight: 700;
		}


	</style>

	<body>
		<div class="container">
			<h1>Summer@ICERM 2020: Graph Spectral Clustering</h1>
			<table style="width: 80%">
				<tr>
					<td>
					<b>Max Daniels</b> <br />
					<span style="font-size: 12px; color: #808080">
						Northeastern Univeristy
						<br />
						daniels.g@northeastern.edu
					</span>
					</td>
					<td>
					<b>Catherine Huang</b> <br />
					<span style="font-size: 12px; color: #808080">
						University of California, Berkeley
						<br />
						thecatherinehuang@berkeley.edu
					</span>
					</td>
					<td>
					<b>Shubham Makaria</b> <br />
					<span style="font-size: 12px; color: #808080">
						Brown University	
						<br />
						shubham@brown.edu
					</span>
					</td>
					<td>
					<b>Chloe Makdad</b> <br />
					<span style="font-size: 12px; color: #808080">
						Butler University	
						<br />
						cmakdad@butler.edu
					</span>
					</td>
				</tr>
			</table>
		<br />
		<p>Welcome to the project page for our Summer@ICERM 2020 project! </p>
		
		<p> We study the Graph Spectral Clustering algorithm, an alternative to Euclidean clustering algorithms like K-Means, which is geometrically motivated for use on data with manifold structure. It is often challenging to cluster manifold structured data because the Euclidean distance between any two points may not take into account their "true" distance along a geodesic path within the data manifold. A useful rule of thumb is that in high dimensional settings, only local Euclidean distances are meaningful. </p>

		<p>Spectral clustering takes this into account by operating on a similarity graph derived from the data. Edges in the graph represent data similarity and are typically computed with a kernel function like RBF or the exponential kernel. After obtaining the similarity graph, Graph Spectral Clustering utilizes a derived matrix, the <em>Graph Laplacian</em>.</p>

		<p>In this work, we study the role of the Graph Laplacian and connect Graph Spectral Clustering to the behavior of <em>heat diffusion</em>, which is governed by a second order PDE involving the Laplacian operator. For more information, please see <a href="#">our report</a>!</p>
		
		<p><em>We thank our project supervisors, Akil Narayan and Minah Oh, as well as our graduate TA supervisors, Alex Mihai and Liu Yang. We also thank our peers at ICERM for useful discussion, and all of the ICERM organizers for a hosting a wonderful program.</em></p>
		<h1>Demonstrations</h1>
		<img width="80%" style="margin: 2% 10%" src="html/demo/Eigenvector_Evolution_BrokenCircle.gif" />
		<p style="margin: 0% 10%;font-size: 12px; color: #808080" width="80%">The domain is a circle graph with two equidistant low-weight edges. In the first row, we show consecutive eigenvectors of the Graph Laplacian. In the second row, we show how diffusion of random initial conditions converges to eigenvectors of the Graph Laplacian. Different columns show the solution projected onto increasingly high eigenvectors (ie. "high frequencies"). The third row shows energy remaining after this projection.</p>
		<br />
		<p>You can see our work in some of the following Jupyter notebooks:
			<ul>
				<li><p>We explored how graph spectral clustering works.  </p>
				<ul>
					<li><a href="html/Cumulative Eigenvector Clustering.html">Cumulative Eigenvector Clustering:</a> Graphs showing how eigenvectors help with picking out clusters and gives an example where GSC outperforms k-means. 				
					<li><a href="html/Random Walks Between Clusters.html">Random Walks Between Clusters</a> <p> Test: Graphs showing how eigenvectors help with picking out clusters and gives an example where GSC outperforms k-means.</p>
				</ul>
				
				<li> <p>We clustered some real world datasets. </p>
				<ul>
					<li><a href="html/Mushroom_Clustering.html">Mushroom_Clustering</a>
					<li><a href="html/Logistic Regression for Feature Selection.html">Logistic Regression for Feature Selection</a>
					<li><a href="html/GSC for Codenames.html">GSC for Codenames</a>
				</ul>
				<li> <p>Since the graph Laplacian is intimately related to the Laplace operator, 
				<li> <p>We looked at some physical interpretations of graph spectral clustering with respects to the heat equation. </p>
				<ul>
					<li><a href="html/Inverse Laplacian and the Heat Kernel.html">Inverse Laplacian and the Heat Kernel</a>
					<li><a href="html/Evolving Eigenvectors under the Heat Equation.html">Evolving Eigenvectors under the Heat Equation</a>
					<li><a href="html/Clustering and Image Blurs.html">Clustering and Image Blurs</a>
					<li><a href="html/1D Heat Diffusion on a Rod.html">1D Heat Diffusion on a Rod</a>

				</ul>
				
				<li><p>We also looked at the wave equation and investigated how evolving a graph according to the wave equation could allow us to recover eigenvectors. </p>
				<ul>
					<li><a href="html/Hearing the Clusters of a Graph.html">Hearing the Clusters of a Graph</a>
				</ul>
			</ul>
		</p>
		</div>

		
	</body>

</html>
